{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "            builder. \\\n",
    "            master('local'). \\\n",
    "            appName(\"practice\"). \\\n",
    "            getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b7bd6954c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading and writing files using pyspark local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading and writing files using pyspark HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = spark.read.csv(r'C:\\Users\\Kshitij kadu\\Desktop\\sample_data\\emp\\Records.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp ID: integer (nullable = true)\n",
      " |-- Name Prefix: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Middle Initial: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- E Mail: string (nullable = true)\n",
      " |-- Father's Name: string (nullable = true)\n",
      " |-- Mother's Name: string (nullable = true)\n",
      " |-- Mother's Maiden Name: string (nullable = true)\n",
      " |-- Date of Birth: string (nullable = true)\n",
      " |-- Time of Birth: string (nullable = true)\n",
      " |-- Age in Yrs.: double (nullable = true)\n",
      " |-- Weight in Kgs.: integer (nullable = true)\n",
      " |-- Date of Joining: string (nullable = true)\n",
      " |-- Quarter of Joining: string (nullable = true)\n",
      " |-- Half of Joining: string (nullable = true)\n",
      " |-- Year of Joining: integer (nullable = true)\n",
      " |-- Month of Joining: integer (nullable = true)\n",
      " |-- Month Name of Joining: string (nullable = true)\n",
      " |-- Short Month: string (nullable = true)\n",
      " |-- Day of Joining: integer (nullable = true)\n",
      " |-- DOW of Joining: string (nullable = true)\n",
      " |-- Short DOW: string (nullable = true)\n",
      " |-- Age in Company (Years): double (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Last % Hike: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- Phone No. : string (nullable = true)\n",
      " |-- Place Name: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- User Name: string (nullable = true)\n",
      " |-- Password: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_detail = emp_df.select(\"Emp ID\", \"First Name\", \"Last Name\", \"Gender\", \"Date of Birth\", \"Salary\",\"County\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp ID: integer (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Date of Birth: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_detail.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_detail.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_detail.repartition(5).write.csv(r'C:\\Users\\Kshitij kadu\\Desktop\\retail\\write\\write_csv\\try',mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              County|count|\n",
      "+--------------------+-----+\n",
      "|               Bucks|   14|\n",
      "|              Grimes|    2|\n",
      "|           Worcester|   21|\n",
      "|            Thurston|    3|\n",
      "|Skagway-Hoonah-An...|    2|\n",
      "|      East Feliciana|    1|\n",
      "|                Utah|    2|\n",
      "|               Tyler|    1|\n",
      "|             Hanover|    2|\n",
      "|          Charleston|   10|\n",
      "|             Shannon|    3|\n",
      "|             Whigham|    1|\n",
      "|            Montcalm|    5|\n",
      "|                 Bow|    1|\n",
      "|               Pasco|    8|\n",
      "|            Angelina|    3|\n",
      "|              Hawaii|    6|\n",
      "|               Izard|    3|\n",
      "|          Deer Lodge|    1|\n",
      "|            Harrison|   17|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_detail.groupBy('County').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9995"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(r\"C:\\Users\\Kshitij kadu\\Desktop\\retail\\write\\write_csv\\try\",header = True, inferSchema=True).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = spark.read.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/order*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.repartition(5).write.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/write/order_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df1 = spark.read.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/write/order_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68883"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df = spark.read.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/order*\")\n",
    "order_df.count()\n",
    "order_df.repartition(5).write.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/write/order_json\", mode = 'overwrite')\n",
    "order_df1 = spark.read.json(\"hdfs://localhost:9000/user/Kshitij kadu/userfirst/write/order_json\")\n",
    "order_df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method take in module pyspark.sql.dataframe:\n",
      "\n",
      "take(num) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.take(2)\n",
      "    [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(order_df1.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(order_customer_id=8652, order_date='2013-12-28 00:00:00.0', order_id=25286, order_status='COMPLETE'),\n",
       " Row(order_customer_id=5754, order_date='2014-07-18 00:00:00.0', order_id=56525, order_status='PENDING_PAYMENT'),\n",
       " Row(order_customer_id=11028, order_date='2014-03-10 00:00:00.0', order_id=63741, order_status='PROCESSING'),\n",
       " Row(order_customer_id=5672, order_date='2013-12-15 00:00:00.0', order_id=61465, order_status='PROCESSING'),\n",
       " Row(order_customer_id=736, order_date='2014-07-04 00:00:00.0', order_id=54540, order_status='PENDING_PAYMENT'),\n",
       " Row(order_customer_id=11900, order_date='2013-08-11 00:00:00.0', order_id=3070, order_status='PENDING'),\n",
       " Row(order_customer_id=2997, order_date='2014-01-17 00:00:00.0', order_id=68022, order_status='COMPLETE'),\n",
       " Row(order_customer_id=5929, order_date='2013-12-02 00:00:00.0', order_id=67866, order_status='PENDING'),\n",
       " Row(order_customer_id=6460, order_date='2013-09-04 00:00:00.0', order_id=6559, order_status='CLOSED'),\n",
       " Row(order_customer_id=7970, order_date='2013-09-03 00:00:00.0', order_id=58814, order_status='PROCESSING')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
